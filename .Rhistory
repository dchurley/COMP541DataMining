graph_from_data_frame(d = word_correlations, vertices = author_used_word) %>%
ggraph(layout = "fr") + geom_edge_link() +
geom_node_point() + geom_node_text(aes(label = name))
graph_from_data_frame(d = word_correlations,
vertices = author_used_word %>%
semi_join(word_correlations, by = c("word" = "item1"))) %>%
ggraph(layout = "fr") + geom_edge_link() +
geom_node_point() + geom_node_text(aes(label = name))
# Load the text data
library(RMySQL)
conn = dbConnect(RMySQL::MySQL(),
dbname='project',
host='150.230.44.118',
port=3306,
user='project',
password='COMP541',
local_infile = TRUE)
fullData = dbGetQuery(conn, statement = "select * from `Articles` where `id` < 100")
news_words <- fullData %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word, "[:alpha:]")) %>%
distinct()
author_used_word <- news_words %>%
count(word, name = "authors_n") %>%
filter(authors_n >= 10)
word_correlations <- news_words %>%
semi_join(author_used_word, by = "word") %>%
pairwise_cor(item = word, feature = author) %>%
filter(correlation >= 0.2)
author_used_word <- news_words %>%
count(word, name = "authors_n") %>%
filter(authors_n >= 10)
word_correlations <- news_words %>%
semi_join(author_used_word, by = "word") %>%
pairwise_cor(item = word, feature = author) %>%
filter(correlation >= 0.2)
graph_from_data_frame(d = word_correlations,
vertices = author_used_word %>%
semi_join(word_correlations, by = c("word" = "item1"))) %>%
ggraph(layout = "fr") + geom_edge_link() +
geom_node_point() + geom_node_text(aes(label = name))
graph_from_data_frame(d = word_correlations,
vertices = author_used_word %>%
semi_join(word_correlations, by = c("word" = "item1"))) %>%
ggraph(layout = "fr") + geom_edge_link() +
geom_node_point() + geom_node_text(aes(label = name), repel = TRUE)
author_used_word <- news_words %>%
count(word, name = "authors_n") %>%
filter(authors_n >= 30)
word_correlations <- news_words %>%
semi_join(author_used_word, by = "word") %>%
pairwise_cor(item = word, feature = author) %>%
filter(correlation >= 0.2)
graph_from_data_frame(d = word_correlations,
vertices = author_used_word %>%
semi_join(word_correlations, by = c("word" = "item1"))) %>%
ggraph(layout = "fr") + geom_edge_link() +
geom_node_point() + geom_node_text(aes(label = name), repel = TRUE)
# Load the text data
library(RMySQL)
conn = dbConnect(RMySQL::MySQL(),
dbname='project',
host='150.230.44.118',
port=3306,
user='project',
password='COMP541',
local_infile = TRUE)
fullData = dbGetQuery(conn, statement = "select * from `Articles`")
news_words <- fullData %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word, "[:alpha:]")) %>%
distinct()
author_used_word <- news_words %>%
count(word, name = "authors_n") %>%
filter(authors_n >= 50)
word_correlations <- news_words %>%
semi_join(author_used_word, by = "word") %>%
pairwise_cor(item = word, feature = author) %>%
filter(correlation >= 0.3)
graph_from_data_frame(d = word_correlations,
vertices = author_used_word %>%
semi_join(word_correlations, by = c("word" = "item1"))) %>%
ggraph(layout = "fr") + geom_edge_link() +
geom_node_point() + geom_node_text(aes(label = name), repel = TRUE)
author_used_word <- news_words %>%
count(word, name = "authors_n") %>%
filter(authors_n >= 50)
word_correlations <- news_words %>%
semi_join(author_used_word, by = "word") %>%
pairwise_cor(item = word, feature = author) %>%
filter(correlation >= 0.6)
graph_from_data_frame(d = word_correlations,
vertices = author_used_word %>%
semi_join(word_correlations, by = c("word" = "item1"))) %>%
ggraph(layout = "fr") + geom_edge_link() +
geom_node_point() + geom_node_text(aes(label = name), repel = TRUE)
#DOCUMENT TERM matrix for LDA
dtm <- DocumentTermMatrix(corpus)
#dtm <- removeSparseTerms(dtm, sparse = 0.1)
# Filter out documents with no words
dtm <- dtm[rowSums(as.matrix(dtm)) > 0, ]
#this takes a minute to run
num_topics <- 5
lda_model <- LDA(dtm, k = num_topics, method = "Gibbs")
#this takes a minute to run
num_topics <- 10
lda_model <- LDA(dtm, k = num_topics, method = "Gibbs")
#func to remove everything but english characters and spaces
removeSpecialChars <- function(x) {
gsub("[^a-zA-Z ]", "", x)
}
# Create a corpus
corpus <- Corpus(VectorSource(fullData$text))
# clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removeSpecialChars))
corpus <- tm_map(corpus, removeWords, stopwords())
corpus <- tm_map(corpus, stripWhitespace)
# TERM DOCUMENT matrix for word cloud
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
#DOCUMENT TERM matrix for LDA
dtm <- DocumentTermMatrix(corpus)
#dtm <- removeSparseTerms(dtm, sparse = 0.1)
# Filter out documents with no words
dtm <- dtm[rowSums(as.matrix(dtm)) > 0, ]
#this takes a minute to run
num_topics <- 10
lda_model <- LDA(dtm, k = num_topics, method = "Gibbs")
# Get the terms associated with each topic
terms <- terms(lda_model, 10)  # Adjust the number of terms as needed
# Print the top terms for each topic
for (i in 1:num_topics) {
cat("Topic", i, ":", paste(terms[i, ], collapse = ", "), "\n")
}
#func to remove everything but english characters and spaces
removeSpecialChars <- function(x) {
gsub("[^a-zA-Z ]", "", x)
}
# Create a corpus
corpus <- Corpus(VectorSource(fullData$text))
# clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removeSpecialChars))
corpus <- tm_map(corpus, removeWords, stopwords())
corpus <- tm_map(corpus, removeWords, "said")
corpus <- tm_map(corpus, stripWhitespace)
# TERM DOCUMENT matrix for word cloud
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
#this takes a long time to run
suppressWarnings(wordcloud(d$word, d$freq, random.order=FALSE, rot.per=0))
#DOCUMENT TERM matrix for LDA
dtm <- DocumentTermMatrix(corpus)
#dtm <- removeSparseTerms(dtm, sparse = 0.1)
# Filter out documents with no words
dtm <- dtm[rowSums(as.matrix(dtm)) > 0, ]
#this takes a minute to run
num_topics <- 10
lda_model <- LDA(dtm, k = num_topics, method = "Gibbs")
# Get the terms associated with each topic
terms <- terms(lda_model, 10)  # Adjust the number of terms as needed
# Print the top terms for each topic
for (i in 1:num_topics) {
cat("Topic", i, ":", paste(terms[i, ], collapse = ", "), "\n")
}
library(tm)
library(wordcloud)
library(topicmodels)
library(tidyverse)
library(tidytext)
library(widyr)
library(ggraph)
library(igraph)
# Load the text data
library(RMySQL)
conn = dbConnect(RMySQL::MySQL(),
dbname='project',
host='150.230.44.118',
port=3306,
user='project',
password='COMP541',
local_infile = TRUE)
fullData = dbGetQuery(conn, statement = "select * from `Articles` where `source` = 'New York Times'")
#format and clean the data
news_words <- fullData %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word, "[:alpha:]")) %>%
distinct()
#filter out words not used by at least 50 authors
#and not creating at least 60% correlation
author_used_word <- news_words %>%
count(word, name = "authors_n") %>%
filter(authors_n >= 50)
word_correlations <- news_words %>%
semi_join(author_used_word, by = "word") %>%
pairwise_cor(item = word, feature = author) %>%
filter(correlation >= 0.6)
#func to remove everything but english characters and spaces
removeSpecialChars <- function(x) {
gsub("[^a-zA-Z ]", "", x)
}
# Create a corpus
corpus <- Corpus(VectorSource(fullData$text))
# clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removeSpecialChars))
corpus <- tm_map(corpus, removeWords, stopwords())
#remove the word said because it's the most common and it's boring
corpus <- tm_map(corpus, removeWords, "said")
corpus <- tm_map(corpus, stripWhitespace)
# TERM DOCUMENT matrix for word cloud
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
#this takes a long time to run
suppressWarnings(wordcloud(d$word, d$freq, random.order=FALSE, rot.per=0))
library(tm)
library(wordcloud)
library(topicmodels)
library(tidyverse)
library(tidytext)
library(widyr)
library(ggraph)
library(igraph)
# Load the text data
library(RMySQL)
conn = dbConnect(RMySQL::MySQL(),
dbname='project',
host='150.230.44.118',
port=3306,
user='project',
password='COMP541',
local_infile = TRUE)
fullData = dbGetQuery(conn, statement = "select * from `Articles` where `source` = 'New York Times'")
#func to remove everything but english characters and spaces
removeSpecialChars <- function(x) {
gsub("[^a-zA-Z ]", "", x)
}
# Create a corpus
corpus <- Corpus(VectorSource(fullData$text))
# clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removeSpecialChars))
corpus <- tm_map(corpus, removeWords, stopwords())
#remove the word said because it's the most common and it's boring
corpus <- tm_map(corpus, removeWords, "said")
corpus <- tm_map(corpus, stripWhitespace)
# TERM DOCUMENT matrix for word cloud
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
#this takes a long time to run
suppressWarnings(wordcloud(d$word, d$freq, random.order=FALSE, rot.per=0))
gc()
library(tm)
library(wordcloud)
library(topicmodels)
# Load the text data
library(RMySQL)
conn = dbConnect(RMySQL::MySQL(),
dbname='project',
host='150.230.44.118',
port=3306,
user='project',
password='COMP541',
local_infile = TRUE)
fullData = dbGetQuery(conn, statement = "select * from `Articles` where `source` = 'New York Times'")
# Load the text data
library(RMySQL)
conn = dbConnect(RMySQL::MySQL(),
dbname='project',
host='150.230.44.118',
port=3306,
user='project',
password='COMP541',
local_infile = TRUE)
fullData = dbGetQuery(conn, statement = "select * from `Articles` where `source` like 'New York Times'")
# Load the text data
library(RMySQL)
conn = dbConnect(RMySQL::MySQL(),
dbname='project',
host='150.230.44.118',
port=3306,
user='project',
password='COMP541',
local_infile = TRUE)
fullData = dbGetQuery(conn, statement = "SELECT * FROM `Articles` WHERE `source` LIKE 'New York Times'")
# Load the text data
library(RMySQL)
conn = dbConnect(RMySQL::MySQL(),
dbname='project',
host='150.230.44.118',
port=3306,
user='project',
password='COMP541',
local_infile = TRUE)
fullData = dbGetQuery(conn, statement = "SELECT * FROM `Articles` WHERE `source` LIKE '%York%'")
# Load the text data
library(RMySQL)
conn = dbConnect(RMySQL::MySQL(),
dbname='project',
host='150.230.44.118',
port=3306,
user='project',
password='COMP541',
local_infile = TRUE)
fullData = dbGetQuery(conn, statement = "SELECT * FROM `Articles` WHERE `source` LIKE '%New York Times%'")
#func to remove everything but english characters and spaces
removeSpecialChars <- function(x) {
gsub("[^a-zA-Z ]", "", x)
}
# Create a corpus
corpus <- Corpus(VectorSource(fullData$text))
# clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removeSpecialChars))
corpus <- tm_map(corpus, removeWords, stopwords())
#remove the word said because it's the most common and it's boring
corpus <- tm_map(corpus, removeWords, "said")
corpus <- tm_map(corpus, stripWhitespace)
# TERM DOCUMENT matrix for word cloud
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
#this takes a long time to run
suppressWarnings(wordcloud(d$word, d$freq, random.order=FALSE, rot.per=0))
library(tm)
library(wordcloud)
library(topicmodels)
# Load the text data
library(RMySQL)
conn = dbConnect(RMySQL::MySQL(),
dbname='project',
host='150.230.44.118',
port=3306,
user='project',
password='COMP541',
local_infile = TRUE)
fullData = dbGetQuery(conn, statement = "SELECT * FROM `Articles` WHERE `source` LIKE '%Fox News%'")
#func to remove everything but english characters and spaces
removeSpecialChars <- function(x) {
gsub("[^a-zA-Z ]", "", x)
}
# Create a corpus
corpus <- Corpus(VectorSource(fullData$text))
# clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removeSpecialChars))
corpus <- tm_map(corpus, removeWords, stopwords())
#remove the word said because it's the most common and it's boring
corpus <- tm_map(corpus, removeWords, "said")
corpus <- tm_map(corpus, stripWhitespace)
# TERM DOCUMENT matrix for word cloud
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
#this takes a long time to run
suppressWarnings(wordcloud(d$word, d$freq, random.order=FALSE, rot.per=0))
library(tm)
library(wordcloud)
library(topicmodels)
#this takes a minute to run
num_topics <- 5
lda_model <- LDA(dtm, k = num_topics, method = "Gibbs")
#DOCUMENT TERM matrix for LDA
dtm <- DocumentTermMatrix(corpus)
#dtm <- removeSparseTerms(dtm, sparse = 0.1)
# Filter out documents with no words
dtm <- dtm[rowSums(as.matrix(dtm)) > 0, ]
#this takes a minute to run
num_topics <- 5
lda_model <- LDA(dtm, k = num_topics, method = "Gibbs")
# Get the terms associated with each topic
terms <- terms(lda_model, 3)  # Adjust the number of terms as needed
# Print the top terms for each topic
for (i in 1:num_topics) {
cat("Topic", i, ":", paste(terms[i, ], collapse = ", "), "\n")
}
#this takes a minute to run
num_topics <- 5
lda_model <- LDA(dtm, k = num_topics, method = "Gibbs")
# Get the terms associated with each topic
terms <- terms(lda_model, 5)  # Adjust the number of terms as needed
# Print the top terms for each topic
for (i in 1:num_topics) {
cat("Topic", i, ":", paste(terms[i, ], collapse = ", "), "\n")
}
# Get the terms associated with each topic
terms <- terms(lda_model, 10)  # Adjust the number of terms as needed
# Print the top terms for each topic
for (i in 1:num_topics) {
cat("Topic", i, ":", paste(terms[i, ], collapse = ", "), "\n")
}
#this takes a minute to run
num_topics <- 15
lda_model <- LDA(dtm, k = num_topics, method = "Gibbs")
# Get the terms associated with each topic
terms <- terms(lda_model, 10)  # Adjust the number of terms as needed
# Print the top terms for each topic
for (i in 1:num_topics) {
cat("Topic", i, ":", paste(terms[i, ], collapse = ", "), "\n")
}
# Get the terms associated with each topic
terms <- terms(lda_model, 5)  # Adjust the number of terms as needed
# Print the top terms for each topic
for (i in 1:num_topics) {
cat("Topic", i, ":", paste(terms[i, ], collapse = ", "), "\n")
}
# Get the terms associated with each topic
terms <- terms(lda_model, 15)  # Adjust the number of terms as needed
# Print the top terms for each topic
for (i in 1:num_topics) {
cat("Topic", i, ":", paste(terms[i, ], collapse = ", "), "\n")
}
#this takes a minute to run
num_topics <- 5
lda_model <- LDA(dtm, k = num_topics, method = "Gibbs")
# Get the terms associated with each topic
terms <- terms(lda_model, num_topics)  # Adjust the number of terms as needed
# Print the top terms for each topic
for (i in 1:num_topics) {
cat("Topic", i, ":", paste(terms[i, ], collapse = ", "), "\n")
}
install.packages("ggplot2")
# Get the dominant topic for each document
dominant_topic <- apply(lda_model$topics, 1, which.max)
library(tm)
library(wordcloud)
library(topicmodels)
library(tidyverse)
library(tidytext)
library(widyr)
library(ggraph)
library(ggplot2)
library(igraph)
# Load the text data
library(RMySQL)
conn = dbConnect(RMySQL::MySQL(),
dbname='project',
host='150.230.44.118',
port=3306,
user='project',
password='COMP541',
local_infile = TRUE)
fullData = dbGetQuery(conn, statement = "select * from `Articles`")
#func to remove everything but english characters and spaces
removeSpecialChars <- function(x) {
gsub("[^a-zA-Z ]", "", x)
}
# Create a corpus
corpus <- Corpus(VectorSource(fullData$text))
# clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removeSpecialChars))
corpus <- tm_map(corpus, removeWords, stopwords())
#remove the word said because it's the most common and it's boring
corpus <- tm_map(corpus, removeWords, "said")
corpus <- tm_map(corpus, stripWhitespace)
# TERM DOCUMENT matrix for word cloud
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
#DOCUMENT TERM matrix for LDA
dtm <- DocumentTermMatrix(corpus)
#dtm <- removeSparseTerms(dtm, sparse = 0.1)
# Filter out documents with no words
dtm <- dtm[rowSums(as.matrix(dtm)) > 0, ]
#this takes a minute to run
num_topics <- 5
lda_model <- LDA(dtm, k = num_topics, method = "Gibbs")
# Get the terms associated with each topic
terms <- terms(lda_model, num_topics)  # Adjust the number of terms as needed
# Print the top terms for each topic
for (i in 1:num_topics) {
cat("Topic", i, ":", paste(terms[i, ], collapse = ", "), "\n")
}
# Get the terms associated with each topic
terms <- terms(lda_model, num_topics)  # Adjust the number of terms as needed
# Print the top terms for each topic
for (i in 1:num_topics) {
cat("Topic", i, ":", paste(terms[i, ], collapse = ", "), "\n")
}
# Get the topic distributions for each document
doc_topics <- posterior(lda_model)$topics
# Display the topic distribution for a specific document (e.g., document 1)
print(doc_topics[1, ])
# Get the dominant topic for each document
dominant_topic <- apply(lda_model$topics, 1, which.max)
# Get the dominant topic for each document
dominant_topic <- apply(doc_topics, 1, which.max)
# Count the number of articles per topic
topic_counts <- table(dominant_topic)
# Convert to data frame
topic_counts_df <- data.frame(Topic = as.integer(names(topic_counts)), Count = as.numeric(topic_counts))
# Plot the graph
ggplot(topic_counts_df, aes(x = factor(Topic), y = Count)) +
geom_bar(stat = "identity") +
labs(x = "Topic", y = "Number of Articles") +
ggtitle("Number of Articles per Topic")
