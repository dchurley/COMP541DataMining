{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1d5317-7339-4aed-b978-164907cce217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import mysql.connector\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "577dd0d9-888c-42a6-b255-0a94b9c694b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvec = TfidfVectorizer()\n",
    "LR = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0902bc-b3c2-40ac-80f8-7b3a9a44ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input is not-cleaned documents that are labeled 0 for fake and 1 for real\n",
    "#must be dataframe with columns 'text' and 'label'\n",
    "#This function cleans the text and tests the model\n",
    "def test_pipeline(data):\n",
    "    #clean the text\n",
    "    data['text'] = data['text'].apply(clean_text)\n",
    "    #vectorize it\n",
    "    xv = tfidfvec.transform(data['text'])\n",
    "    #test the vector data against the label\n",
    "    score = LR.score(xv, data['label'])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ab5e58-c8e0-4cac-86ad-da0df4c00aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input is not-cleaned documents that are labeled 0 for fake and 1 for real\n",
    "#must be dataframe with columns 'text' and 'label'\n",
    "#This function cleans the text and trains the model\n",
    "def train_pipeline(data):\n",
    "    #clean the text\n",
    "    data['text'] = data['text'].apply(clean_text)\n",
    "    #vectorize it\n",
    "    xv = tfidfvec.fit_transform(data['text'])\n",
    "    #fit the vector data to the label\n",
    "    LR.fit(xv, data['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f539d46-ca97-424c-a8bf-cdde4e4f7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get a query from our specific database and return it as a dataframe\n",
    "def get_sql_data(query):\n",
    "    # MySQL info\n",
    "    config = {\n",
    "        'user': 'project',\n",
    "        'password': 'COMP541',\n",
    "        'host': '150.230.44.118',\n",
    "        'database': 'project'\n",
    "    }\n",
    "    #connect\n",
    "    conn = mysql.connector.connect(**config)\n",
    "    #read into dataframe\n",
    "    data = pd.read_sql(query, conn)\n",
    "    #close connection\n",
    "    conn.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba9d719c-1521-4614-88b6-514cfa4d81db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dchur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dchur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "#clean text works on one document at a time so needs to be applied with .apply()\n",
    "def clean_text(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove special characters and convert to lowercase\n",
    "    clean_tokens = [re.sub(r'[^a-zA-Z\\s]', '', token).lower() for token in tokens]\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('said')\n",
    "    stop_words.add('reuters')\n",
    "    clean_tokens = [token for token in clean_tokens if token not in stop_words]\n",
    "    # Join tokens back into a single string\n",
    "    clean_text = ' '.join(clean_tokens)\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7448219-a32f-499a-b149-53bc43785a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dchur\\AppData\\Local\\Temp\\ipykernel_14672\\1325120582.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data = pd.read_sql(query, conn)\n",
      "C:\\Users\\dchur\\AppData\\Local\\Temp\\ipykernel_14672\\1325120582.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "#import training data\n",
    "query = \"SELECT `text`,`label` FROM `Tutorial`\"\n",
    "data = get_sql_data(query)\n",
    "query = \"SELECT `text` FROM `Articles` WHERE `source` LIKE '%New York Times%'\"\n",
    "nyt_data = get_sql_data(query)\n",
    "#the nyt is highly trusted, it's label is always real\n",
    "nyt_data['label'] = 1\n",
    "\n",
    "\n",
    "#import testing data\n",
    "query = \"SELECT `text`,`label` FROM `Kaggle`\"\n",
    "kaggle = get_sql_data(query)\n",
    "#sample for testing\n",
    "#data = data.sample(1000)\n",
    "# Invert the label column for Kaggle set\n",
    "kaggle['label'] = kaggle['label'].map({0: 1, 1: 0})\n",
    "\n",
    "\n",
    "#concat the nyt articles\n",
    "data = pd.concat([data, nyt_data, kaggle], ignore_index=True)\n",
    "#sample for testing\n",
    "#data = data.sample(1000)\n",
    "\n",
    "#split into test and train\n",
    "data_train, data_test = train_test_split(data, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6217377e-7eec-41ea-b80e-3a53fef0dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15f96e50-cf87-4655-b385-03b2d3f57977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9563144000681257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pipeline(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "519a4977-627c-4b14-b3c6-f14cbad4e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_pipeline(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
