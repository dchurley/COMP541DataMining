```{r}
library(tm)
library(wordcloud)
library(topicmodels)
library(tidyverse)
library(tidytext)
library(widyr)
library(ggraph)
library(ggplot2)
library(igraph)
```

``` {r}
# Load the text data
library(RMySQL)
conn = dbConnect(RMySQL::MySQL(),
                            dbname='project',
                            host='150.230.44.118',
                            port=3306,
                            user='project',
                            password='COMP541',
                            local_infile = TRUE)

fullData = dbGetQuery(conn, statement = "select * from `Articles`")
```


```{r}
#NETWORK GRAPH CODE
#
#format and clean the data for the network graph
news_words <- fullData %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(str_detect(word, "[:alpha:]")) %>%
  distinct()
```

```{r}
#filter out words not used by at least 50 authors
#and not creating at least 60% correlation
author_used_word <- news_words %>%
  count(word, name = "authors_n") %>%
  filter(authors_n >= 50)

word_correlations <- news_words %>%
  semi_join(author_used_word, by = "word") %>%
  pairwise_cor(item = word, feature = author) %>%
  filter(correlation >= 0.6)
```

```{r}
#graph the data
graph_from_data_frame(d = word_correlations, 
                      vertices = author_used_word %>%
                        semi_join(word_correlations, by = c("word" = "item1"))) %>%
  
  ggraph(layout = "fr") + geom_edge_link() + 
  geom_node_point() + geom_node_text(aes(label = name), repel = TRUE)
```







```{r}
#func to remove everything but english characters and spaces
removeSpecialChars <- function(x) {
  gsub("[^a-zA-Z ]", "", x)
}

# Create a corpus
corpus <- Corpus(VectorSource(fullData$text))
# clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removeSpecialChars))
corpus <- tm_map(corpus, removeWords, stopwords())
#remove the word said because it's the most common and it's boring
corpus <- tm_map(corpus, removeWords, "said")
corpus <- tm_map(corpus, stripWhitespace)
```

```{r}
# TERM DOCUMENT matrix for word cloud
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
```

```{r}
#this takes a long time to run
suppressWarnings(wordcloud(d$word, d$freq, random.order=FALSE, rot.per=0))
```
```{r}
#DOCUMENT TERM matrix for LDA
dtm <- DocumentTermMatrix(corpus)
#dtm <- removeSparseTerms(dtm, sparse = 0.1)
# Filter out documents with no words
dtm <- dtm[rowSums(as.matrix(dtm)) > 0, ]
```

```{r}
#this takes a minute to run
num_topics <- 5
lda_model <- LDA(dtm, k = num_topics, method = "Gibbs")
```

```{r}
# Get the terms associated with each topic
terms <- terms(lda_model, num_topics)  # Adjust the number of terms as needed

# Print the top terms for each topic
for (i in 1:num_topics) {
  cat("Topic", i, ":", paste(terms[i, ], collapse = ", "), "\n")
}
```
```{r}
# Get the topic distributions for each document
doc_topics <- posterior(lda_model)$topics

# Display the topic distribution for a specific document (e.g., document 1)
print(doc_topics[1, ])
```

```{r}
# Get the dominant topic for each document
dominant_topic <- apply(doc_topics, 1, which.max)

# Count the number of articles per topic
topic_counts <- table(dominant_topic)

# Convert to data frame
topic_counts_df <- data.frame(Topic = as.integer(names(topic_counts)), Count = as.numeric(topic_counts))

# Plot the graph
ggplot(topic_counts_df, aes(x = factor(Topic), y = Count)) +
  geom_bar(stat = "identity") +
  labs(x = "Topic", y = "Number of Articles") +
  ggtitle("Number of Articles per Topic")
```




