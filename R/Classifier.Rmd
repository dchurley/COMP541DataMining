```{r}
#import libraries
library(tidytext)
library(caret)
library(tm)
library(e1071)
```


```{r}
#get the training data
source("Database.R")

data <- SQL_GET_DATA("Tutorial")
```

```{r}
#func to remove all characters but english alphabet letters
removeSpecialChars <- function(x) {
  gsub("[^a-zA-Z ]", "", x)
}


# Create a corpus from text data
corpus <- Corpus(VectorSource(data$text))

# clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removeSpecialChars))
corpus <- tm_map(corpus, removeWords, stopwords())
#remove the word said because it's the most common and it's boring
corpus <- tm_map(corpus, removeWords, c("said", "reuters"))
corpus <- tm_map(corpus, stripWhitespace)
#corpus <- tm_map(corpus, stemDocument, language="english")
#creating term matrix with TF-IDF weighting
dtm <-DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
dtm
```



```{r}
spam <- removeSparseTerms(dtm, 0.90)
spam
```


```{r}
dataFrame <- as.data.frame(as.matrix(spam), stringsAsFactors=False)
dataFrame$label = data$label
dataFrame
```



```{r}
library(rpart)
# Split data into training and testing sets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(dataFrame$label, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- dataFrame[trainIndex, ]
test_data <- dataFrame[-trainIndex, ]

# Train a classifier
model <- rpart(label ~ ., data = train_data, method = "class")

# Make predictions on test data
predictions <- predict(model, newdata = test_data, type = "class")

# Evaluate the model
confusion_matrix <- table(predictions, test_data$label)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
```
```{r}
confusion_matrix
```

```{r}
word_list <- colnames(dataFrame)
words_to_remove <- c("label")
word_list <- word_list[!word_list %in% words_to_remove]



kaggle <- SQL_GET_DATA("Kaggle")
#invert the label column since the kaggle dataset uses 1 for fake and 0 for real
kaggle$label <- ifelse(kaggle$label == 0, 1, 0)

corpus <- Corpus(VectorSource(kaggle$text))

# clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removeSpecialChars))
corpus <- tm_map(corpus, removeWords, stopwords())
#remove the word said because it's the most common and it's boring
corpus <- tm_map(corpus, removeWords, c("said", "reuters"))
corpus <- tm_map(corpus, stripWhitespace)
#corpus <- tm_map(corpus, stemDocument, language="english")
#creating term matrix with TF-IDF weighting
dtm <-DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE), dictionary = word_list))
```

```{r}
dataFrame <- as.data.frame(as.matrix(dtm), stringsAsFactors=False)
dataFrame$label = kaggle$label
dataFrame
```
```{r}
# Make predictions on test data
predictions <- predict(model, newdata = dataFrame, type = "class")

# Evaluate the model
confusion_matrix <- table(predictions, dataFrame$label)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
confusion_matrix
```

